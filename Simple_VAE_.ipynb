{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggZxg3fAWr-r"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "from imgaug import augmenters \n",
    "from numpy import loadtxt\n",
    "from pylab import rcParams\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oeawj-vmXJ9L"
   },
   "source": [
    "# LOAD DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7AvhRELrWxZy"
   },
   "outputs": [],
   "source": [
    "def load_dataset(batch_size=1000):\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "    test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "    # Normalizing the images to the range of [0., 1.]\n",
    "    train_images /= 255.\n",
    "    test_images /= 255.\n",
    "\n",
    "    TRAIN_BUF = 60000\n",
    "    TEST_BUF = 10000\n",
    "\n",
    "    BATCH_SIZE = batch_size\n",
    "    # convert dataset to batches of data \n",
    "    train_dataset_image = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)\n",
    "    train_dataset_label = tf.data.Dataset.from_tensor_slices(train_labels).batch(BATCH_SIZE)\n",
    "    train_dataset = tf.data.Dataset.zip((train_dataset_image, train_dataset_label)).shuffle(TRAIN_BUF)\n",
    "\n",
    "    test_dataset_image = tf.data.Dataset.from_tensor_slices(test_images).batch(BATCH_SIZE)\n",
    "    test_dataset_label = tf.data.Dataset.from_tensor_slices(test_labels).batch(BATCH_SIZE)\n",
    "    test_dataset = tf.data.Dataset.zip((test_dataset_image, test_dataset_label)).shuffle(TEST_BUF)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fw2t7gGysMwz"
   },
   "source": [
    "# PLOT GENERATED IMAGES AND LATENT SPACE DISTRIBUTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8D-L0JwsU3E"
   },
   "outputs": [],
   "source": [
    "# regenerate images from test set \n",
    "# 5 indicative images are reconstructed \n",
    "def plot_reconstructed_test_data(model, test_dataset):\n",
    "    n = 5\n",
    "    sample_dataset = test_dataset\n",
    "    x_input, y_input = next(sample_dataset.__iter__())\n",
    "    x_input_sample, y_input_sample = map(lambda x: x[:n], (x_input, y_input)) \n",
    "    z = model.encode(x_input_sample)[0].numpy()\n",
    "\n",
    "    fig, axarr = plt.subplots(2, 5, figsize=(10, 5))\n",
    "    x_input_sample = x_input_sample.numpy().reshape([n, 28, 28])\n",
    "    x_output = model.decode(z, apply_sigmoid=True).numpy().reshape([n, 28, 28])\n",
    "\n",
    "    for i in range(n):\n",
    "        axarr[0, i].axis('off')\n",
    "        axarr[1, i].axis('off')\n",
    "        axarr[0, i].imshow(x_input_sample[i], cmap='binary')\n",
    "        axarr[1, i].imshow(x_output[i], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXRauMGWscXx"
   },
   "outputs": [],
   "source": [
    "# latent space distribution \n",
    "# lor latent dim 2 \n",
    "def latent_space_distribution(model, test_dataset):\n",
    "    n = 5\n",
    "    sample_dataset = test_dataset\n",
    "    x_input, y_input = next(sample_dataset.__iter__())\n",
    "    x_input_sample, y_input_sample = map(lambda x: x[:n], (x_input, y_input))\n",
    "    z, _ = model.encode(x_input)\n",
    "    labels = y_input.numpy()\n",
    "    z1, z2 = z.numpy().T[0], z.numpy().T[1]\n",
    "\n",
    "    colors = matplotlib.cm.rainbow(np.linspace(0, 1, 10))\n",
    "    cs = [colors[y] for y in labels]\n",
    "    classes = list(range(10))\n",
    "\n",
    "    recs = []\n",
    "    for i in range(0, len(cs)):\n",
    "        recs.append(mpatches.Rectangle((0, 0), 1, 1, fc=cs[i]))\n",
    "\n",
    "    fig_dist = plt.figure(figsize=(8, 8))\n",
    "    ax_dist = fig_dist.add_subplot(111)\n",
    "    ax_dist.legend(recs, classes, loc=0)\n",
    "    ax_dist.scatter(z1, z2, color=cs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bfzww_YVdw5I"
   },
   "outputs": [],
   "source": [
    "# generate images giving to the decoder a latent vector \n",
    "def generated_images(model, test_dataset):\n",
    "    n = 20\n",
    "    f, axarr = plt.subplots(n, n, figsize=(12, 12))\n",
    "    f.subplots_adjust(hspace=0., wspace=-0.)\n",
    "    for i, z1 in enumerate(np.linspace(-2, 2, n)):\n",
    "        for j, z2 in enumerate(np.linspace(-2, 2, n)):\n",
    "            z = np.array([[z1, z2]])\n",
    "            generated_img = model.decode(z, apply_sigmoid=True).numpy().reshape([28, 28])\n",
    "            axarr[i, j].axis('off')\n",
    "            axarr[i, j].imshow(generated_img, cmap='binary')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwY606v5tGs-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_VAE(model, test_dataset):\n",
    "  plot_reconstructed_test_data(model, test_dataset)\n",
    "  latent_space_distribution(model, test_dataset)\n",
    "  generated_images(model, test_dataset)\n",
    "\n",
    "  n = 16\n",
    "  num_classes = 10\n",
    "  f, axarr = plt.subplots(num_classes, n, figsize=(n, num_classes))\n",
    "  f.subplots_adjust(hspace=0., wspace=-0.)\n",
    "  for i in range(num_classes):\n",
    "      for j, z_j in enumerate(np.linspace(-2, 2, n)):\n",
    "          z = np.array([[z_j, 0]])\n",
    "          z = tf.convert_to_tensor(z, dtype=tf.float32)\n",
    "          generated_img = model.decode(z, apply_sigmoid=True).numpy().reshape([28, 28])\n",
    "          axarr[i, j].axis('off')\n",
    "          axarr[i, j].imshow(generated_img, cmap='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZu_WoFgXMu8"
   },
   "source": [
    "# MODEL VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhLYAes-XQuV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def encoder (latent_dim):\n",
    "        encoder=tf.keras.Sequential([\n",
    "            # input layer : image                                                       \n",
    "            tf.keras.layers.InputLayer(input_shape=[28, 28, 1]),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # 2 dense layers \n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            # output layer : for each latent variable we are\n",
    "            # going to canculate its mean and variance                 \n",
    "            tf.keras.layers.Dense(latent_dim * 2),    # [means, stds]\n",
    "        ])\n",
    "        return encoder\n",
    "\n",
    "def decoder (latent_dim):\n",
    "      decoder=tf.keras.Sequential([\n",
    "            # input layer : a value for each latent variable  \n",
    "            tf.keras.layers.InputLayer(input_shape=[latent_dim]),\n",
    "            # 2 dense layers \n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            # calculate and reconstruct the output to the shape shape as input image \n",
    "            tf.keras.layers.Dense(28 * 28 * 1),\n",
    "            tf.keras.layers.Reshape(target_shape=[28, 28, 1]),\n",
    "      ])\n",
    "      return decoder\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = encoder(self.latent_dim )\n",
    "        self.generative_net = decoder(self.latent_dim)\n",
    "  \n",
    "    def encode(self, x):\n",
    "        # call encoder\n",
    "        mean_logvar = self.inference_net(x)\n",
    "        N = mean_logvar.shape[0]\n",
    "        # First outpout of encoder: the mean of the distribution q(zi|x) \n",
    "        mean = tf.slice(mean_logvar, [0, 0], [N, self.latent_dim])\n",
    "        # Second outpout of encoder: the covariance table calculated for the distribution q(zi|x)  \n",
    "        logvar = tf.slice(mean_logvar, [0, self.latent_dim], [N, self.latent_dim])\n",
    "        return mean, logvar\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        else:            \n",
    "            return logits\n",
    "\n",
    "    # function used for reparametrization trick ( mean + conv * eps) \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNmu01yoXXxd"
   },
   "source": [
    "# TRAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3ROuL82XZY6"
   },
   "outputs": [],
   "source": [
    "# For training our algorithm we have to maximize the ELBO \n",
    "# Instead of that convert this maximization problem to a minimization problem \n",
    "# by calculating the -ELBO\n",
    "\n",
    "class VAETrain:\n",
    "    @staticmethod\n",
    "    def compute_loss(model, x):\n",
    "        mean, logvar = model.encode(x)\n",
    "        z = model.reparameterize(mean, logvar)\n",
    "        x_logits = model.decode(z)\n",
    "\n",
    "        # cross_ent = - marginal likelihood\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logits, labels=x)\n",
    "        marginal_likelihood = - tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "        marginal_likelihood = tf.reduce_mean(marginal_likelihood)\n",
    "\n",
    "        # Our KL divergence loss can be rewritten in the formula defined above (Wiseodd, 2016).\n",
    "        # https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/\n",
    "        KL_divergence = tf.reduce_sum(mean ** 2 + tf.exp(logvar) - logvar - 1, axis=1)\n",
    "        KL_divergence = tf.reduce_mean(KL_divergence)\n",
    "\n",
    "        ELBO = marginal_likelihood - KL_divergence\n",
    "        # Adam optimizer used finds the min of a function \n",
    "        # for this reason we convert ELBO (mazimization problem)\n",
    "        # to -ELBO \n",
    "        # now using as a loos function = -ELBO we convert the initial maximization project \n",
    "        # to a minimization problem\n",
    "        loss = -ELBO\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_gradients(model, x, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = VAETrain.compute_loss(model, x)\n",
    "        loss_init = loss\n",
    "        gradients= tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss_init\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mS4ec1qdvIJ"
   },
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5Eg7pbqf941"
   },
   "outputs": [],
   "source": [
    "def train_VAE(latent_dim=2, epochs=100, lr=1e-4, batch_size=1000):\n",
    "    model = VAE(latent_dim)\n",
    "    train_dataset, test_dataset = load_dataset(batch_size=batch_size)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    # for each epoch train all the batches \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t = time.time()\n",
    "        last_loss = 0\n",
    "        for train_x, _ in train_dataset:\n",
    "            loss = VAETrain.compute_gradients(model, train_x, optimizer)\n",
    "            last_loss = loss\n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch {}, Loss: {}, Remaining Time at This Epoch: {:.2f}'.format(\n",
    "                epoch, last_loss, time.time() - t\n",
    "            ))\n",
    "\n",
    "    plot_VAE(model, test_dataset)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7rsWSY0W6GJ"
   },
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LX9LLYNFgAgh",
    "outputId": "fbc6089b-8cdc-4b64-a858-c7ddcdfd18a7"
   },
   "outputs": [],
   "source": [
    "train_VAE(latent_dim=2,epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "k_b8H3XcV168",
    "outputId": "9f490eb5-2927-4ce9-a994-bbe7228e7c5e"
   },
   "outputs": [],
   "source": [
    "train_VAE(latent_dim=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wByi4vjr0YSU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Simple_VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
