{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggZxg3fAWr-r"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "\n",
    "from IPython import display\n",
    "from imgaug import augmenters \n",
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "from pylab import rcParams\n",
    "import numpy \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aq1UyZOzJRAa"
   },
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_RtTGNUJSmE"
   },
   "outputs": [],
   "source": [
    "# regenerate images from test set \n",
    "# 5 indicative images are reconstructed \n",
    "def plot_reconstructed_test_data(model, test_dataset):\n",
    "    n = 5\n",
    "    sample_dataset = test_dataset\n",
    "    x_input, y_input = next(sample_dataset.__iter__())\n",
    "    x_input_sample, y_input_sample = map(lambda x: x[:n], (x_input, y_input))\n",
    "    z = model.encode(x_input_sample, y_input_sample)[0].numpy()\n",
    "\n",
    "    fig, axarr = plt.subplots(2, n, figsize=(5, 2))\n",
    "    x_input_sample = x_input_sample.numpy().reshape([n, 28, 28])\n",
    "    x_output = model.decode(z, y_input_sample, apply_sigmoid=True).numpy().reshape([n, 28, 28])\n",
    "\n",
    "    for i in range(n):\n",
    "        axarr[0, i].axis('off')\n",
    "        axarr[1, i].axis('off')\n",
    "        axarr[0, i].imshow(x_input_sample[i], cmap='binary')\n",
    "        axarr[1, i].imshow(x_output[i], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSOD5dQQJS0d"
   },
   "outputs": [],
   "source": [
    "# generate images giving to the decoder a latent vector \n",
    "def generated_images(model, test_dataset):\n",
    "      # image generation \n",
    "      n = 16\n",
    "      num_classes = 10\n",
    "      f, axarr = plt.subplots(num_classes, n, figsize=(n, num_classes))\n",
    "      f.subplots_adjust(hspace=0., wspace=-0.)\n",
    "      for i in range(num_classes):\n",
    "          for j, z_j in enumerate(np.linspace(-2, 2, n)):\n",
    "              z = np.array([[z_j, 0]])\n",
    "              z = tf.convert_to_tensor(z, dtype=tf.float32)\n",
    "              generated_img = model.decode(z, [i], apply_sigmoid=True).numpy().reshape([28, 28])\n",
    "              axarr[i, j].axis('off')\n",
    "              axarr[i, j].imshow(generated_img, cmap='binary')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hi7_iDJtJS5t"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_VAE(model, test_dataset):\n",
    "  plot_reconstructed_test_data(model, test_dataset)\n",
    "  generated_images(model, test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oeawj-vmXJ9L"
   },
   "source": [
    "# LOAD DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7AvhRELrWxZy"
   },
   "outputs": [],
   "source": [
    "def load_dataset(batch_size=1000):\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "    # add noise in initial dataset \n",
    "    noise = augmenters.SaltAndPepper(0.2)\n",
    "    seq_object = augmenters.Sequential([noise])\n",
    "    # normalize input data pixels \n",
    "    train_images = seq_object.augment_images(train_images * 255) / 255\n",
    "    test_images = seq_object.augment_images(test_images * 255) / 255\n",
    "    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "    test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "    # train_images, test_images = normalize(train_images, test_images)\n",
    "\n",
    "    TRAIN_BUF = 60000\n",
    "    TEST_BUF = 10000\n",
    "\n",
    "    BATCH_SIZE = batch_size\n",
    "\n",
    "    train_dataset_image = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)\n",
    "    train_dataset_label = tf.data.Dataset.from_tensor_slices(train_labels).batch(BATCH_SIZE)\n",
    "    train_dataset = tf.data.Dataset.zip((train_dataset_image, train_dataset_label)).shuffle(TRAIN_BUF)\n",
    "\n",
    "    test_dataset_image = tf.data.Dataset.from_tensor_slices(test_images).batch(BATCH_SIZE)\n",
    "    test_dataset_label = tf.data.Dataset.from_tensor_slices(test_labels).batch(BATCH_SIZE)\n",
    "    test_dataset = tf.data.Dataset.zip((test_dataset_image, test_dataset_label)).shuffle(TEST_BUF)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZu_WoFgXMu8"
   },
   "source": [
    "# MODEL Conditional Variational Autoencoder  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhLYAes-XQuV"
   },
   "outputs": [],
   "source": [
    "def encoder (latent_dim, num_classes):\n",
    "        encoder=tf.keras.Sequential([\n",
    "            # input layer : image  + label                                                   \n",
    "            tf.keras.layers.InputLayer(input_shape=[28 * 28 * 1 + num_classes]),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(latent_dim * 2),    # [means, stds]\n",
    "        ])\n",
    "        return encoder\n",
    "\n",
    "def decoder (latent_dim, num_classes):\n",
    "      decoder=tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=[latent_dim + num_classes]),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(28 * 28 * 1),\n",
    "            tf.keras.layers.Reshape(target_shape=[28, 28, 1]),\n",
    "        ])\n",
    "      return decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuedOOMpJfkK"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = 10\n",
    "        self.inference_net = encoder (self.latent_dim, self.num_classes)\n",
    "        self.generative_net = decoder (self.latent_dim, self.num_classes)\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        conditional_x = tf.concat([Flatten()(x), tf.one_hot(y, self.num_classes)], 1)\n",
    "        mean_logvar = self.inference_net(conditional_x)\n",
    "        N = mean_logvar.shape[0]\n",
    "        mean = tf.slice(mean_logvar, [0, 0], [N, self.latent_dim])\n",
    "        logvar = tf.slice(mean_logvar, [0, self.latent_dim], [N, self.latent_dim])\n",
    "        return mean, logvar\n",
    "\n",
    "    def decode(self, z, y, apply_sigmoid=False):\n",
    "        conditional_z = tf.concat([Flatten()(z), tf.one_hot(y, self.num_classes)], 1)\n",
    "        logits = self.generative_net(conditional_z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        else :\n",
    "            return logits\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNmu01yoXXxd"
   },
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAiIftWDQUxl"
   },
   "outputs": [],
   "source": [
    "# For training our algorithm we have to maximize the ELBO \n",
    "# Instead of that convert this maximization problem to a minimization problem \n",
    "# by calculating the -ELBO\n",
    "\n",
    "class CVAETrain:\n",
    "    @staticmethod\n",
    "    def compute_loss(model, x, y):\n",
    "        mean, logvar = model.encode(x, y)\n",
    "        z = model.reparameterize(mean, logvar)\n",
    "        x_logits = model.decode(z, y)\n",
    "\n",
    "        # cross_ent = - marginal likelihood\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logits, labels=x)\n",
    "        marginal_likelihood = - tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "        marginal_likelihood = tf.reduce_mean(marginal_likelihood)\n",
    "\n",
    "\n",
    "        # Our KL divergence loss can be rewritten in the formula defined above (Wiseodd, 2016).\n",
    "        # https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/\n",
    "        KL_divergence = tf.reduce_sum(mean ** 2 + tf.exp(logvar) - logvar - 1, axis=1)\n",
    "        KL_divergence = tf.reduce_mean(KL_divergence)\n",
    "        \n",
    "        # Adam optimizer used finds the min of a function \n",
    "        # for this reason we convert ELBO (mazimization problem)\n",
    "        # to -ELBO \n",
    "        # now using as a loos function = -ELBO we convert the initial maximization project \n",
    "        # to a minimization problem\n",
    "        ELBO = marginal_likelihood - KL_divergence\n",
    "        loss = -ELBO\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_gradients(model, x,y, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = CVAETrain.compute_loss(model, x, y)\n",
    "        loss_init = loss\n",
    "        gradients= tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S17SUWitJmjj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_CVAE(latent_dim=2, epochs=100, lr=1e-4, batch_size=1000):\n",
    "    model = CVAE(latent_dim)\n",
    "    # load and normalize data , split it to batches \n",
    "    train_dataset, test_dataset = load_dataset(batch_size=batch_size)\n",
    "    # initialize Adam optimiser necessary for minimizing Loss Function \n",
    "    optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        last_loss = 0\n",
    "        t = time.time()\n",
    "        # For each epoch we train our model for the whole dataset ( all batches ) \n",
    "        for train_x, train_y in train_dataset:\n",
    "            loss = CVAETrain.compute_gradients(model, train_x, train_y, optimizer)\n",
    "            last_loss = loss\n",
    "        if epoch % 50 == 0:\n",
    "            print('Epoch {}, Loss: {}, Remaining Time at This Epoch: {:.2f}'.format(\n",
    "                epoch, last_loss, time.time() - t\n",
    "            ))\n",
    "\n",
    "    plot_VAE(model, test_dataset)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckxNGEGhdfhX"
   },
   "source": [
    "# Denoising task Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "o3ROuL82XZY6",
    "outputId": "b83aa88e-9297-4bf1-bb53-9792451a2ebb"
   },
   "outputs": [],
   "source": [
    "train_CVAE(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "colab_type": "code",
    "id": "wfYWOa9OXzku",
    "outputId": "2fdba126-b461-4578-ba2c-f374ff852c26"
   },
   "outputs": [],
   "source": [
    "train_CVAE(epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rk1m5paZkDF_"
   },
   "source": [
    "# Higher dimention Latent space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "xcnMthVEYYSu",
    "outputId": "b1c7fee9-e6d8-4584-8109-8c3ea87da096"
   },
   "outputs": [],
   "source": [
    "def plot_CVAE(model, test_dataset):\n",
    "    n = 10\n",
    "    sample_dataset = test_dataset\n",
    "    x_input, y_input = next(sample_dataset.__iter__())\n",
    "    x_input_sample, y_input_sample = map(lambda x: x[:n], (x_input, y_input))\n",
    "    z = model.encode(x_input_sample, y_input_sample)[0].numpy()\n",
    "\n",
    "    fig, axarr = plt.subplots(2, n, figsize=(10,4))\n",
    "    x_input_sample = x_input_sample.numpy().reshape([n, 28, 28])\n",
    "    x_output = model.decode(z, y_input_sample, apply_sigmoid=True).numpy().reshape([n, 28, 28])\n",
    "\n",
    "    for i in range(n):\n",
    "        axarr[0, i].axis('off')\n",
    "        axarr[1, i].axis('off')\n",
    "        axarr[0, i].imshow(x_input_sample[i], cmap='binary')\n",
    "        axarr[1, i].imshow(x_output[i], cmap='binary')\n",
    "\n",
    "\n",
    "\n",
    "train_CVAE(latent_dim=10,epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8la6q1OR28wx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Denoising.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
